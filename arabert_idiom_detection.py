# -*- coding: utf-8 -*-
"""arabert_idiom_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12Kgi6BKhTLch-FBhWB4uKBRjioCCJM1N
"""

import pandas as pd
df = pd.read_csv('df.csv')

df.shape

import nltk

print ("nbre total de mots : ",len (df.	word_form))
word_dist = nltk.FreqDist(df.	word_form)
print ("vocabulaire :", len (word_dist))

df

# Calcul de la moitié de la longueur du DataFrame
moitie = len(df) // 2

# Division du DataFrame en deux parties
df1 = df.iloc[:moitie]
df2 = df.iloc[moitie:]

df1.shape



!pip install transformers

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Bidirectional, Dense, Conv1D, MaxPooling1D, GlobalMaxPool1D
from tensorflow.keras.initializers import glorot_uniform
#from arabert import ArabertPreprocessor
from transformers import AutoTokenizer, AutoModel

from transformers import BertTokenizer, TFBertModel

"""**Tokenization**"""

tokenizer = BertTokenizer.from_pretrained('aubmindlab/bert-base-arabertv02')
max_length = 128

from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Model
from transformers import TFBertModel
from tensorflow.keras.layers import Input
from tensorflow.keras.layers import GlobalMaxPooling1D
from tensorflow.keras.layers import Dense
X_train, X_test, y_train, y_test = train_test_split(df1['word_form'], df1["mwe_tag"], test_size=0.2)
X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size=0.2)

# Tokenize and pad train, dev, and test sentences
train_tokenized_inputs = tokenizer(X_train.tolist(), padding='max_length', truncation=True, max_length=max_length, return_tensors='np')
X_train = np.array(train_tokenized_inputs['input_ids'])
y_train = np.array(y_train.tolist())
dev_tokenized_inputs = tokenizer(X_dev.tolist(), padding='max_length', truncation=True, max_length=max_length, return_tensors='np')
X_dev = np.array(dev_tokenized_inputs['input_ids'])
y_dev = np.array(y_dev.tolist())
test_tokenized_inputs = tokenizer(X_test.tolist(), padding='max_length', truncation=True, max_length=max_length, return_tensors='np')
X_test = np.array(test_tokenized_inputs['input_ids'])
y_test = np.array(y_test.tolist())

""" **Modeling**"""

# Couche d'entrée pour les données textuelles
input_layer = Input(shape=(max_length,), dtype='int32')

# Modèle BERT pré-entraîné en arabe (non entraînable)
bert_model = TFBertModel.from_pretrained('aubmindlab/bert-base-arabertv02', trainable=False)

# Utilisation du modèle BERT pour traiter les données d'entrée
bert_output = bert_model(input_layer)

# Accès à la sortie de la couche de pool (CLS token) de BERT
pooled_output = bert_output.pooler_output

# Ajout d'une couche de convolution
convolution_layer = Conv1D(filters=64, kernel_size=3, activation='relu')(bert_output.last_hidden_state)

# Couche BiLSTM
lstm_layer = Bidirectional(LSTM(64, return_sequences=True))(convolution_layer)


# Couche d'agrégation globale (ou GlobalAveragePooling1D)
pooling_layer = GlobalMaxPooling1D()(lstm_layer)

# Ajout de couches de sortie pour la classification multiclasse
output_layer = Dense(2, activation='softmax')(pooling_layer)

# Création du modèle global
model = Model(inputs=input_layer, outputs=output_layer)

# Compilation du modèle
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'], run_eagerly=True)

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
y_test = label_encoder.transform(y_test)
y_dev = label_encoder.transform(y_dev)

from keras.utils import to_categorical

y_train = to_categorical(y_train, num_classes=2)
y_test = to_categorical(y_test, num_classes=2)
y_dev = to_categorical(y_dev, num_classes=2)

history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_dev, y_dev))

predicted_bi_lstm = model.predict(X_test)
predicted_bi_lstm

print(y_test.shape)
print(predicted_bi_lstm.shape)

predicted_bi_lstm=np.argmax(predicted_bi_lstm, axis=1)
y_test=np.argmax(y_test, axis=1)

"""**Evaluation**"""

import sklearn
from sklearn.metrics import precision_recall_fscore_support as score

precision, recall, fscore, support = score(y_test, predicted_bi_lstm)

print('precision: {}'.format(precision))
print('recall: {}'.format(recall))
print('fscore: {}'.format(fscore))
print('support: {}'.format(support))
print('################################')
print(sklearn.metrics.classification_report(y_test, predicted_bi_lstm))

"""**Model Assessment**"""

import matplotlib.pyplot as plt
def accuracy_plot(history):

    fig, ax = plt.subplots(1, 2, figsize=(12,5))

    fig.suptitle('Model Performance with Epochs', fontsize = 16)
    # Subplot 1
    ax[0].plot(history.history['accuracy'])
    ax[0].plot(history.history['val_accuracy'])
    ax[0].set_title('Model Accuracy', fontsize = 14)
    ax[0].set_xlabel('Epochs', fontsize = 12)
    ax[0].set_ylabel('Accuracy', fontsize = 12)
    ax[0].legend(['train', 'validation'], loc='best')

    # Subplot 2
    ax[1].plot(history.history['loss'])
    ax[1].plot(history.history['val_loss'])
    ax[1].set_title('Model Loss', fontsize = 14)
    ax[1].set_xlabel('Epochs', fontsize = 12)
    ax[1].set_ylabel('Loss', fontsize = 12)
    ax[1].legend(['train', 'validation'], loc='best')


accuracy_plot(history)

import seaborn as sns
from sklearn.metrics import confusion_matrix
def plot_cm(model, X_test, y_test):
    mwe = ['Idiom', 'Normal']

    # Declaring confusion matrix
    cm = confusion_matrix(y_test, predicted_bi_lstm)

    # Heat map labels
    group_counts = ['{0:0.0f}'.format(value) for value in cm.flatten()]
    group_percentages = ['{0:.2%}'.format(value) for value in cm.flatten()/np.sum(cm)]

    labels = [f"{v2}\n{v3}" for v2, v3 in zip(group_counts, group_percentages)]
    labels = np.asarray(labels).reshape(5,5)

    # Plotting confusion matrix
    plt.figure(figsize=(12,8))
    sns.heatmap(cm, cmap=plt.cm.Blues, annot=labels, annot_kws={"size": 15}, fmt = '',
                xticklabels = mwe,
                yticklabels = mwe)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12, rotation='horizontal')
    plt.title('Confusion Matrix\n', fontsize=19)
    plt.xlabel('Predicted Labels', fontsize=17)
    plt.ylabel('Actual Labels', fontsize=17)

plot_cm(model, X_test, y_test)